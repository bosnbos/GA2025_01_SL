---
title: "NB_Logistic"
author: "Bos Noah"
date: "`r Sys.Date()`"
output: github_document

---

```{r train test split}
set.seed(4500393)
df = read.csv('spambase.csv')
idx = sample(1:nrow(df),size = 0.7*nrow(df), replace = F)

```

# Lasso, ridge or relaxed lassso?

Given that some predictors appear to have weak effects on *CLASS* and the number of predictors is moderate (k = 58), regularized models like Ridge, Lasso, and Elastic Net are good choices. Standard logistic regression is not viable in this setting due to the high-dimensional structure, which can lead to perfect separation—a scenario where a hyperplane classifies the training data perfectly, preventing the model from converging (see below). Lasso may introduce bias through shrinkage, but its variance reduction can improve test-set performance. Relaxed Lasso is also used to refit an OLS model on selected variables, reducing bias while maintaining sparsity.

- **Logistic regression** is a reasonable baseline, but it can perform poorly when predictors are highly correlated or when the number of variables is large. In such cases, it may overfit or fail to converge due to perfect separation.
- **Ridge regression** is better suited to datasets with many correlated predictors. It retains all variables but shrinks their coefficients, reducing overfitting and potentially improving predictive accuracy.
- **Lasso regression** performs automatic feature selection by shrinking some coefficients to zero. It is most effective when we believe that only a subset of variables truly contributes to the outcome, making it ideal for building sparse, interpretable models.
- **Relaxed Lasso** may be preferred when Lasso is too aggressive. After selecting variables, it reduces bias by partially refitting coefficients, often improving generalization without sacrificing sparsity.



```{r}
hist(cor(df), breaks = 58, xlim = c(-0.5, 0.9))
```

- slight multicollinearity come correlations are quite large

```{r standard logistic regression, eval=FALSE, }
#library(caret)
#cv_ctrl = trainControl(method = "cv", number = 10, classProbs = TRUE, summaryFunction = twoClassSummary)
#df$spam = factor(df$Class, levels = c(0,1), label = c('not_spam', "spam"))


#cv_logistic = train(
#  spam ~ ., data = df[idx,],
#  method = "glm",
#  family = "binomial",
#  trControl = cv_ctrl,
#  metric = "ROC",
#  preProcess = c("center", "scale", "zv")
#)
```

Model found a hyperplane that separates the classes perfectly, coefs go to ± infinity. common in text data with word frequencies. Too many predictors Logistic regression overfits or cant solve the likelihood equation. Perfect is simply more likely with higher dimensions

```{r data prep}
library(glmnet)
X = as.matrix(subset(df, select = -Class))
y = as.matrix(df$Class)


```

```{r lasso}
# Lasso (alpha = 1)
cv_lasso <- cv.glmnet(X[idx,], y[idx], family = "binomial", alpha = 1)
plot(cv_lasso)
cv_lasso
```
Plot does not have convex cure for the CV error. The minumum CV binomial deviance yields a model of 62. Lambda 1 SE criterion suggest sparser solution of 44 predictors which we use.

```{r ridge}
# Ridge (alpha = 0)
cv_ridge = cv.glmnet(X[idx,], y[idx], family = "binomial", alpha = 0)
plot(cv_ridge)
cv_ridge
```
Ridge, as expected, selects all predictor variables. The cross-validated Ridge regression ($\alpha =0$) achieved minimum binomial deviance of 0.5229 at $\lambda$ = 0.01811, using all 57 predictors. The 1-SE rule suggests a slightly larger $\lambda$ (0.02394) with comparable performance (deviance = 0.5354), offering a more regularized, stable model without variable elimination.sa

```{r}
hist(coef(cv_ridge)[,1], breaks = 58, main = 'Distribution ridge coeficients \n most coef are 0, some are quite strong ',)
```

This histogram shows the distribution of Ridge regression coefficients. Unlike Lasso, Ridge does not set coefficients exactly to zero, but shrinks them toward zero. Most coefficients are small, clustered around zero, indicating weak predictors, while a few have larger absolute values, suggesting stronger influence on the outcome.

```{r elastic net}
# Elastic Net (alpha = 0.5)
cv_elastic = cv.glmnet(X[idx,], y[idx], family = "binomial", alpha = 0.5)
plot(cv_elastic)
cv_elastic
```
This Elastic Net model ($\alpha = 0.5$ ) achieved its lowest binomial deviance (0.4589) at $\lambda$ = 0.00059 with 56 nonzero coefficients. The 1-SE rule favors a more regularized model at $\lambda$ = 0.00287, slightly increasing deviance to 0.4751 and reducing the model to 54 predictors, improving generalization with minimal loss in fit.

```{r relaxed lasso}
#relaxed lasso
cv_relaxed = cv.glmnet(X[idx,], y[idx], family = "binomial", alpha = 1, relax = T)
plot(cv_relaxed)
cv_relaxed
```

This Relaxed Lasso model achieved its lowest binomial deviance (0.4571) with $\gamma$ = 1 (equivalent to standard Lasso), $\lambda$ = 0.000366, and 53 active predictors. Using the 1-SE rule, a more relaxed model ($\gamma$ = 0.25) with $\lambda$ = 0.0151 selects only 35 predictors and maintains strong performance (deviance = 0.4721), offering improved sparsity and potentially better generalization.

# Model evaluation

```{r Test prediction}
l_preds = predict(cv_lasso, s = "lambda.min", newx = X[-idx,], type = 'response')
r_preds = predict(cv_ridge, s = "lambda.min", newx = X[-idx,], type = 'response')
e_preds = predict(cv_elastic, s = "lambda.min", newx = X[-idx,], type = 'response')
x_preds = predict(cv_relaxed, newx = X[-idx, ], s = "lambda.min", type = "response")
```

```{r probs to class labels}
l_class = ifelse(l_preds > 0.5, 1, 0)
r_class = ifelse(r_preds > 0.5, 1, 0)
e_class = ifelse(e_preds > 0.5, 1, 0)
x_class = ifelse(e_preds > 0.5, 1, 0)
```

```{r confusion matrix and accuracy}
library(caret)
confusionMatrix(factor(l_class), factor(y[-idx]), positive = "1")  # Lasso
confusionMatrix(factor(r_class), factor(y[-idx]), positive = "1")  # Ridge
confusionMatrix(factor(e_class), factor(y[-idx]), positive = "1")  # Elastic Net
confusionMatrix(factor(x_class), factor(y[-idx]), positive = "1")  # relaxed lasso
```

```{r AUC, warning=FALSE, message=FALSE}
library(pROC)

l_auc <- auc(roc(y[-idx], as.numeric(l_preds)))
r_auc <- auc(roc(y[-idx], as.numeric(r_preds)))
e_auc <- auc(roc(y[-idx], as.numeric(e_preds)))
x_auc <- auc(roc(y[-idx], as.numeric(x_preds)))
print(c(Lasso = l_auc, Ridge = r_auc, ElasticNet = e_auc, Relaxed_Lasso = x_auc))
```
These AUC results show that all four models perform well, with AUCs above 0.95. Relaxed Lasso outperforms the others, achieving the highest AUC (0.973), suggesting it best balances variable selection and predictive accuracy. Lasso, Ridge, and Elastic Net perform comparably ($\approx 0.96$), but Relaxed Lasso offers a modest improvement in discriminative power.

```{r roc curves, warning=FALSE, message=FALSE}
roc_l <- roc(y[-idx], as.numeric(l_preds))
roc_r <- roc(y[-idx], as.numeric(r_preds))
roc_e <- roc(y[-idx], as.numeric(e_preds))
roc_x <- roc(y[-idx], as.numeric(x_preds))

plot(roc_l, col = "blue", legacy.axes = TRUE, main = "ROC Curves for Lasso, Ridge, Elastic Net")
plot(roc_r, col = "red", add = TRUE)
plot(roc_e, col = "green", add = TRUE)
plot(roc_x, col = "black", add = TRUE)

legend("bottomright", legend = c(
  paste0("Lasso (AUC = ", round(auc(roc_l), 2), ")"),
  paste0("Ridge (AUC = ", round(auc(roc_r), 2), ")"),
  paste0("Elastic Net (AUC = ", round(auc(roc_e), 2), ")"),
  paste0("Relaxed lasso (AUC = ", round(auc(roc_x), 2), ")")
), col = c("blue", "red", "green", 'black'), lwd = 2)
```


```{r results}
results <- data.frame(
  Model = c("Lasso", "Ridge", "Elastic Net", 'Relaxed Lasso'),
  AUC = c(l_auc, r_auc, e_auc, x_auc),
  Accuracy = c(
    mean(l_class == y[-idx]),
    mean(r_class == y[-idx]),
    mean(e_class == y[-idx]),
    mean(x_class == y[-idx])
  )
)

print(results)
```
**Results**
Among the four models evaluated, Relaxed Lasso achieved the highest AUC (0.973) while maintaining strong accuracy (91.02%), suggesting it balances variable selection and coefficient stability effectively. Lasso also performed well, with a slightly lower AUC (0.962) and the highest accuracy (91.24%). Ridge lagged behind slightly in both metrics (AUC = 0.960, accuracy = 90.22%), indicating less effective regularization. Elastic Net offered a middle ground between Ridge and Lasso in both AUC (0.961) and accuracy (91.02%). Overall, Relaxed Lasso provides the best trade-off between predictive power and generalization, making it the most robust choice for this classification task.



